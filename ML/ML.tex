\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{float}

\title{CS5014: Machine Learning, Revision Notes}
\author{Gergely Flamich}

\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\|}{\,\,|\,\,}
\newcommand{\norm}[1]{\,||#1||\,}
\newcommand{\diff}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}

\DeclareMathOperator*{\sgn}{sgn}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\maketitle

\section{Overview}
In ML, we usually have some data in the following general setup:
\begin{itemize}
\item \textbf{Features}: literally any sort of observations form the real world,
  collected conveniently into a matrix:
  \[
    X =
    \begin{bmatrix}
      x_{1, 1} & x_{1, 2} & \hdots & x_{1, n} \\
      x_{2, 1} & x_{2, 2} & \hdots & x_{2, n} \\
      \vdots & \vdots & \ddots & \vdots \\
      x_{m, 1} & x_{m, 2} & \hdots & x_{m, n}
    \end{bmatrix}
  \]
  where each column $\vec{x}^{(i)}$ contains $m$ observations of the $i$-th feature
\item \textbf{Labels}: In supervised learning we also have another set of data,
  called the ground truth of the object we are trying to predict from the
  features. These are conveniently colleted into a vector $\vec{y}$ with $m$
  elements, each $y_i$ corresponding to a set of observations, row $\vec{x}_i$
  in $X$.
\end{itemize}
The general assumption for supervised learning is that each feature is associated with a random variable
$X_i$ over some distributions and the ground truth is associated with a random
variable $Y$ over some distribution. It is usually assumed that we know the
distributions of $X_i$ and $Y$ is hidden. fianlly, we assume that there is some
function $f$ such that
\[
  Y = f(\vec{X}, \theta) + \epsilon
\]
for some model paramters $\theta$ and some small error term $\epsilon$.
Then, the way we will perform prediction is by using the distribution
\[
  \hat{Y} = f(\vec{X}, \theta).
\]
$f$ is usually referred to as the \textbf{model function}.
\subsection{Machine Learning Pipeline}
\begin{enumerate}
\item Obtain data
\item Clean data
\item Visualise \& Analyse data
\item Prepare data: augumentation, feature selection etc.
\item Perform the learning task
\item Evaluate algorithm
\item Test algorithm
\end{enumerate}
\subsection{Flavours of ML}
Supervised learning:
\begin{itemize}
\item Output is continuous: regression
\item Output is discrete: classification
\end{itemize}

Unsupervised learning: Clustering
\subsection{Common issues in ML}
\begin{itemize}
\item \textbf{Overfitting - High Variance}: The model is too specific to the
  data. It fits it very well, but doesn't generalise well.
\item \textbf{Underfitting - High Bias}: The model is too general, it doesn't
  explain the training data well, and it doesn't explain new data that well either.
\item \textbf{Data Leakage}: Some information from the test set ``leaks'' into
  the training/ validation set, and so the model will be biased to the seen data.
\item \textbf{Dataset bias}: The training or validation set is not
  representative of the general domain of the problem. 
\end{itemize}
\section{Data Preparation Methods}
\begin{itemize}
\item \textbf{Cleaning}: Remove nonsensical/null values, put data in the right
  format
\item \textbf{Feature Extraction}: use raw features and combine/ transform them
  to purify some information to aid learning: requires domain knowledge usually.
\item \textbf{Feature Selection}: Improve interpretability and computational
  feasibility. Remove unnecessary features (based on domain knowledge or
  statistical analysis): univariate tests, forward stepwise selection, backward
  stepwise selection etc.
\item \textbf{One-Hot Encoding}: For classification tasks with $k$ classes, it is commonplace to
  represent different classes as pairwise orthogonal unit vectors in $\Reals^k$.
  This ensures that $\norm{c_i - c_j} = constant$ for every class, i.e. there is
  no bias towards any class. Example:
  \[
    \begin{bmatrix}
      c_1 \\
      c_4 \\
      c_3 \\
      \vdots \\
      c_2
    \end{bmatrix}
    \quad \to \quad
    \begin{bmatrix}
      1 & 0 & 0 & 0 \\
      0 & 0 & 0 & 1 \\
      0 & 0 & 1 & 0 \\
      \vdots & \vdots & \vdots & \vdots \\
      0 & 1 & 0 & 0
    \end{bmatrix}
  \]
\item \textbf{Decorrelation}: remove linear dependencies from the features:
  whitening, PCA.
\item \textbf{Normalisation}:
  \[
    x_i \quad \to \quad \frac{x_i - \min{\vec{x}}}{\max{\vec{x}} - \min{\vec{x}}}.
  \]
\item \textbf{Standardisation}:
  if $X_k$ is assumed to be symmetrically, or even better, normally distributed
  with mean $\mu_k$ and STD $\sigma_k$, this gives better results than normalisation:
  \[
    x_i \quad \to \quad \frac{x_i - \mu_k}{\sigma_k} 
  \]
\item \textbf{Data Augmentation}: artificially generate more data from our
  preexisting set by introducing small modifications to it.
\end{itemize}
\section{General ML Methodology}
\subsection{Regularisation}
\paragraph{Purpose:} Improve the generalisability of the model and prevent
overfitting.
\paragraph{General Idea:} Penalise an individual parameter growing too large.
\paragraph{Example: Squared Loss}
\[
  L(\theta) = \norm{X\theta - \vec{y}}^2 \quad \to \quad J(\theta) = \norm{X\theta
    - \vec{y}}^2 + \lambda\norm{\theta}^2
\]
for some \textbf{regularisation parameter} $\lambda \in \Reals^+$. If we
regularise using the $L_2$ norm of $\theta$, then the algorithm is called
\textbf{ridge regression}, if we use the $L_1$ norm, then it's called \textbf{lasso}.
\subsection{Model Evaluation}
\paragraph{Train-Validation-Test Split}
Common ratios: 60 - 20 - 20, 80 - 10 - 10, etc.
\begin{itemize}
\item \textbf{Training data}: we train our model using the training data
\item \textbf{Validation data}: we evaluate a trained model on the (preferrably)
  unseen validation data according to some metric(s). Based the model
  performance, we can tune hyperparameters/ choose a different model.
\item \textbf{Test data}: Should always be unseen data to avoid bias in the
  trained model towards previously seen data. If this is not done, it
  constitutes data leakage and our results will be meaningless. The results on
  the test data are the results that we usually report, because it tells us how
  well our model generalises.
\end{itemize}
\paragraph{Cross Validation}:
If we don't have enough data to perform a proper train-validation-test split, we
perform cross-validation. We perform a train-test separation, say 80-20.
Then, taking the training data, we split into $k$ equal portions. Then we train
our models $k$ times on $k - 1$ different portions and validate on the one that
was left out. Then, this helps mitigate some of the bias in our dataset. We call
this k-fold CV.
\paragraph{Stratified Sampling}: ensures consistent representation of classes in
training and validation sets, by sampling each subpopulation (class) of the
dataset independently, and then merging the results.
\paragraph{Performance Metrics}:
Regression:
\begin{itemize}
\item Squared error:
  \[
    SE = \norm{\vec{y} - \hat{\vec{y}}}^2_2
  \]
\item Absolute error:
  \[
    AE = \norm{\vec{u} - \hat{\vec{y}}}_1
  \]
\item Coefficient of determination: (sometimes called explained variance)
  \[
    R^2 = 1 - \frac{\sum_i (y_i - \hat{y_i})^2}{\sum_i (y_i - \bar{y})^2}
  \]
\item Root Mean Squared Error:
  \[
    RMSE = \sqrt{\frac{SE}{m}}
  \]
  RMSE is useful, because it has the same units as the data, so we can
  directly observe how well our model is performing.
\end{itemize}
(Binary) Classification:
\begin{itemize}
\item Accuracy:
  \[
    \frac{TP + TN}{P + N}
  \]
\item Error rates - False Positive Rate:
  \[
    FPR = \frac{FP}{TN + FP}
  \]
  False Negative Rate:
  \[
    FNR = \frac{FN}{FN + TP}
  \]
\item Specificity:
  \[
    1 - FPR
  \]
\item Sensitivity:
  \[
    1 - FNR
  \]
\item Precision:
  \[
    \frac{TP}{TP + FP}
  \]
\item Recall:
  \[
    \frac{TP}{TP + FN}
  \]
\item F1 - score:
  \[
    2\frac{Prec \cdot Recall}{Prec + Recall}
  \]
\end{itemize}
The above pairings of classification metrics are ``working against each other'':
one of them increasing will mean a decrease of their pair.
\paragraph{ROC Curve:} Receiver Operation Characteristic Curve: $TPR$ vs $FPR$
\paragraph{Precision-Recall Curve:} duh
\paragraph{Average Precision (AP):} integral of the precision recall curve
\paragraph{Metrics for multiclass classification:}
\begin{itemize}
\item Report One-vs-All scores
\item Confusion Matrix: for $k$ classes it is a $k \times k$ matrix $C =
  [c_{ij}]$, where $c_{ij} = $ \# of how many times an object of class $c_j$ was
  predicted to be class $c_i$. 
\end{itemize}
\section{Regression Methods}
\subsection{Linear Regression}
One of the simplest models $f$ relies on the further assumption that $Y$ depends
linearly on $\vec{X}$. In particular,
\[
  \hat{Y} = f(\vec{X}, \theta) = \vec{X}\theta + b
\]
where $b$ is called the \textbf{bias}. No
Then, we can quantify the goodness-of-fit of the model, by the squared
error, specified by
\[
  L(\theta, b) = \norm{\vec{y} - f(X, \theta, b)}^2 
\]
Then, to find the optimal set of parameters theta, we can differentiate 
\begin{align*}
  \diff{L}{\theta} &= \diff{}{\theta}{\norm{\vec{y} - f(X, \theta, b)}^2 } \\
                   &= 2 \norm{\vec{y} - f(X, \theta, b)}^2 \diff{}{\theta}\norm{\vec{y} - f(X, \theta, b)}  \\
                   &= 2 \norm{\vec{y} - f(X, \theta, b)} \frac{(\vec{y} - f(X, \theta, b))^T}{2\norm{\vec{y} - f(X, \theta, b)}}\diff{}{\theta}\vec{y} - f(X, \theta, b)  \\
                   &= (\vec{y} - X\theta)^T \cdot -X
\end{align*}
Now to find the minimum, set $\diff{L}{\theta} = 0$.
\begin{align*}
  0 &= -\vec{y}^TX + \theta^TX^TX \\
  \vec{y}^TX &= \theta^TX^TX \\
  \vec{y}^TX(X^TX)^{-1} &= \theta^T \\
\end{align*}
Hence
\[
  \theta = (X^TX)^{-1}X^T\vec{y}
\]
The above is called the \textbf{normal equation}.
An alternative way to solve the above problem is \textbf{gradient descent}.
Iterate
\[
  \theta \leftarrow \theta - \alpha \cdot \nabla L
\]
for some appropriate $\alpha \in \Reals$. Then it can be shown that the $\theta$ converges
to the minimal solution. $\alpha$ is called the \textbf{learning rate}.
\paragraph{Regularised Normal Equation}
\[
  \theta = (X^TX + \lambda I)^{-1}X^T
\]
\subsubsection{Basis Expansion}
Provided we have $k$ real features, we can modify our space in which we consider
the linearity of our model. We can transform a set of observations $\vec{x}$ into the new
space through an arbitrary function $h: \Reals^k \to \Reals^n$ by calculating
$h(\vec{x})$.
then, our model changes to:
\[
  f_\theta(X) = X\theta \quad \to \quad f_\theta(X) = h(X)\theta.
\]
Crucially, $f_\theta$ is still linear in the space defined by $h$, but the
components of $h$ may be non-linear!
\paragraph{Special Case:} Polynomial regression: the $h_i$ components of $h$ are
different combinations of the input features.
\subsection{Logistic Regression}
Model the classification problem using the \textbf{logistic function}:
\[
  \sigma_{\theta}(\vec{x}) = \frac{1}{1 + e^{-\theta^T\vec{x}}}\quad\text{ where
  } \vec{x}, \theta \in \Reals^{n \times 1}
\]
It is easy to see that $\sigma_\theta(\vec{x}) \geq 0.5$ iff $\theta^T\vec{x}
\geq 0$ and less than $0.5$ otherwise.
\[
  B = \{\vec{x} \,\,|\,\, \sigma_\theta(\vec{x}) = 0.5 \}
\]
is called the \textbf{decision boundary} of the model.
\paragraph{}
This model allows for \textbf{binary classification}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{img/log_fun}
  \caption{Logistic function $\sigma_{\theta}(\vec{x})$}
\end{figure}
The loss function for logistic regression is rooted in information gain, so $L$
is defined to be the cross-entropy of the data:
\[
  L(\theta) = -\sum_{i = 1}^m y_i\log (f_\theta(\vec{x}_i)) + (1 - y_i)\log(1 - f_\theta(\vec{x}_i))
\]
Optimise this with gradient descent as before.
\subsubsection{Multi-Class Logistic Regression}
\paragraph{One-vs-All}: For $k$ classes train $k$ classifiers: for each class
$c_i$ one class is $c_i$ and one class is everything else. Then, to predict,
take prediction with highest confidence.
\paragraph{One-vs-One}: For $k$ classes train $\binom{k}{2}$ classifier: one for
each pair of $c_i, c_j$ classes. Then, take a majority vote.
\section{Bayesian Decision Theory}
Main idea of Bayesian Decision Theory: minimise expected loss.
\subsection{MLE}
Assume set of distributions: $\{p_\theta \| \theta \in \Theta \}$. \\
Assume the data is an i.i.d. sample of the above family: $X_1, \hdots, X_n \sim
p_\theta$.\\
Then, attempt to estimate the true $\theta$ that generated the sample:
\[
  \theta_{MLE} = \argmax_{\theta \in \Theta} \Prob [ X \| \theta ]
\]
where $\Prob[ X \| \theta]$ is called the \textbf{likelihood function}.
\paragraph{Pros}
\begin{itemize}
\item Easy to compute
\item Interpretable
\item Nice asymptotic properties
\item Invariant under reparametrisation:
  \[
    g(\theta_{MLE}) = (g(\theta))_{MLE}
  \]
\end{itemize}
\paragraph{Cons}
\begin{itemize}
\item Point estimate
\item Overfitting
\end{itemize}
\subsection{MAP}
Similar assumptions to MLE.
Instead of the likelihood function, estimate the \textbf{posterior distribution}
of $\theta$. We wanna estimate $\Prob[X \land \theta]$, we can use Bayes' Rule:
\[
  \Prob[\theta \| X] = \frac{\Prob[X \| \theta] \Prob[\theta]}{\Prob[X]}
\]
here $\Prob[X]$ is constant, so we need to minimise
\[
  \theta_{MAP} = \argmax_{\theta \in \Theta} \Prob[\theta \| X] = \argmax_{\theta \in \Theta}\Prob[X \| \theta] \Prob[\theta]
\]
\paragraph{Pros}
\begin{itemize}
\item Easy to compute
\item Intrerpretable
\item Avoids overfitting by putting a prior on $\theta$ - Regularisation
\item Converges to MLE
\end{itemize}
\paragraph{Cons}
\begin{itemize}
\item Point estimate
\item Not invariant under reparametrisation:
  \[
    g(\theta_{MAP}) \neq (g(\theta))_{MAP}
  \]
\item Must assume prior on $\theta$
\end{itemize}
\subsection{Density Estimation}
\paragraph{Local density estimation}
assume that the density of the data is locally constant. Then for some volume
$V$ around some point $\vec{X} = [X_1, \hdots, X_P]^T$, if we have $K$ samples
in the volume out of a total of $N$ samples, then define
\[
  \Prob[X] = \frac{K}{NV}.
\]
How we pick $K$ and $V$ determines the method.
\paragraph{Histograms:} split up the space into a grid of cubes called
\textbf{bins}. Then, we fixed $V$ and we'll get $K$ by counting up the samples
in every bin.
\paragraph{Pro:} Cheap
\paragraph{Con:} often too crude.
\paragraph{Parzen density estimation:} Given some point $X$, assume that the
density is some local function of nearby samples. So define:
\[
  \Prob[X] = \frac1N \sum_{n = 1}^N \frac1V \cdot K\left( \frac{X - x_n}{h} \right)
\]
where $h$ is the window size, i.e. it controls the locality of the
\textbf{kernel function} $K$.
The kernel functions must be symmetric probability density functions
themselves.\\
Examples:
\begin{itemize}
\item Tophat kernel: $K$ is a uniform density function between $-h/2$ and $h/2$.
\item Gaussian: $K = \mathcal{N}(0, h/2)$.
\item Exponential
\item Linear
\end{itemize}
\subsection{K-Nearest Neighbours}
If we fix $K$ and let $V$ grow according to
\[
  \Prob[X] = \frac{K}{NV}
\]
then we can estimate class likelihoods according to Bayes' Rule:
\[
  \Prob[C_i \| X] = \frac{\Prob[X \| C_i]\Prob[C_i]}{\Prob[X]} = \frac{K_i}{K}
\]
\section{Maximal Margin Classifiers \& SVMs}
\paragraph{Main idea:} Instead of best fit to all of data, try to
\textbf{separate} it as best as possible, so focus more on data close to the margin.
So, maximise margin separating the data.
\subsection{Setup}
Decision function:
\[
  \hat{y} = \sgn(\vec{x}^T\beta + \beta_0)
\]
Distance to the margin:
\[
  y_i(\vec{x}^T\beta + \beta_0) \geq 0\quad \forall i \in \{1, \hdots, N\}
\]
Then, maximise this distance by enforcing it to be greater than some margin $M$:
\[
 \frac{1}{\norm{\beta}} y_i(\vec{x}^T\beta + \beta_0) \geq M
\]
A \textbf{support vector} is any vector that is direcly on the margin, i.e. the
above inequality is an equality. Setting $M = \frac{1}{\norm{\beta}}$:
\[
  y_i(\vec{x}^T\beta + \beta_0) = 1
\]
This is a \textbf{hard margin}, because this is only possible to be solved if
the data is \textbf{separable} and it fails if it isn't.
\subsection{Soft Margin Relaxation}
Allow a few points be on the wrong side. Introduce some slack variables $\xi =
[\xi_1,\hdots, \xi_N]$, that will allow us to let data points cross the margin:
\[
  y_i(x_i^T\beta + \beta_0) \leq 1 \quad \to \quad y_i(x_i^T\beta + \beta_0)
  \leq 1 - \xi_i
\]
And then constrain $\xi$ to not grow too big:
\[
  \xi_i \leq 0, \quad \norm{\xi}_1 \leq const
\]
\paragraph{Note:} $\xi_i \geq 0$: $\vec{x}_i$ is inside margin. $\xi_i > 1$:
$\vec{x}_i$ is miscalssified.
The above can be reformulated to a cost function as:
\[
  J(\beta) = \frac1N \sum_{i = 1}^N \underbrace{(1 - y_i(x_i^T\beta +
    \beta_0))_+}_{\text{hinge loss}} + \underbrace{\lambda \norm{\beta}_2^2}_{\text{regularisation}}
\]
\paragraph{Note:} The correctly classified data points don't contribute to the
loss at all.
\paragraph{Note:} Hinge loss is not differentiable at the hinge point: use
\textbf{subgradient descent}: pick subgradient that minimises loss the most.
\subsection{Kernel Trick}
Turn the soft margin classification problem into an objective function throught
\textbf{Lagrange relaxation}:
\[
  L_p = \underbrace{\frac12 \norm{\beta}^2}_{\text{optimisation objective}} +
  \underbrace{C \sum_{i=1}^N \xi_i}_{\text{slack variable constraint}} -
  \underbrace{\sum_{i = 1}^N \alpha_i(y_i(\vec{x}_i^T\beta + \beta_0 ) - (1 -
    \xi_i))}_{\text{maximal margin condition}} -
  \underbrace{\sum_{i=1}^N \mu_i \xi_i}_{\text{non-negativity condition}}
\]
where $C, \alpha_i, \mu_i$ are called \textbf{Lagrange multipliers}, they
represent the hard constraints.
This is called the \textbf{primal problem} and hard to optimise: turn it into
the \textbf{dual problem}.
We differentiate with respect to $\beta, \beta_0, \xi_i$ and set the derivatives
to 0 since we know that any solution must be a minimum so
\[
  \beta = \sum_{i = 1}^N \alpha_iy_i\vec{x}_i\quad\quad 0 = \sum_{i = 1}^N
  \alpha_iy_i\quad\quad \alpha_i = C - \mu_i
\]
Using these constraints and putting it back into $L_P$ and simplifying, we get
the \textbf{dual function}:
\[
  L_D = \sum_{i = 1}\alpha_i - \frac12 \sum_{i = 1}^N \sum_{j = 1}^N
  \alpha_i\alpha_jy_iy_j\underbrace{x_i^tx_j}_{\text{dot product}}
\]
We can use the dual function to optimise $\beta$.
Similarly, classification, we have
\[
  \beta_0 = \sum_{i = 1}^N \hat{\alpha}_iy_i\langle {\vec{x}, \vec{x}_i} \rangle
\]
We can perform learning and classification if we know the dot product. The idea
then, is to replace the original dot product with different ones: these are
called \textbf{kernels}.
Examples:
\begin{itemize}
\item Polynomial kernel: corresponds to basis expansion
  \[
    K(\vec{x}, \vec{x}') = (1 - \langle {\vec{x}, \vec{x}'} \rangle)^d
  \]
\item Radial Basis Function: Considers the distance as distances of normal distibutions
  \[
    K(\vec{x}, \vec{x}') = \exp(-\gamma\norm{\vec{x} - \vec{x}'}^2)
  \]
\item Neural Network Kernel
\end{itemize}
\section{Neural Networks and Deep Learning}
Feature selection is difficult and requires a lot of domain knowledge: how about
we let the machine do it?
\begin{itemize}
\item Have $L \geq 2$ layers: first layer is the \textbf{input layer}, last
  layer is the \textbf{output layer}. Everything in between is a \textbf{hidden layer}. 
\item Each layer has some $\vec{z}_i \in \Reals^{m_i}$ units. Then, each unit on some layer is
  connected to every unit from the previous layer in a linear fashion:
  \[
    \forall 0 \leq i \leq L-1:\quad\Theta^{(i)} \in \Reals^{m_{i - 1} \times
      m_i}:\quad \vec{z}_i = \vec{a}_{i-1}^T\Theta^{(i)} + \vec{b}_i. 
  \]
  where $\vec{b}_i$ are called the bias of the layer
  Then, we calculate the \textbf{activations} of the units:
  \[
    \vec{a}_i = g(\vec{z}_i)
  \]
  for some $g:\Reals \to \Reals$.
  Usual picks for $g$:
  \begin{itemize}
  \item Logistic function: $\sigma$
  \item Hyperbolic tangent: $\tanh$
  \item Rectified Linear Unit (ReLU): $g(x) = x_+$
  \end{itemize}
\item Then, a single \textbf{forward propagation} of the network can be written
  as:
  \[
    \hat{\vec{y}} = g(\hdots g(g(\vec{x}^T\Theta^{(1)})^T\Theta^{(2)})^T\hdots \Theta^{(L)})
  \]
  This can be used to perform regression or classification.
\item The cost function is
  \[
    J(\Theta) = \frac{1}{m} \sum_{i = 1}^m L(\vec{y}, \hat{\vec{y}}) +
    \frac{\lambda}{2m}\sum_{l = 1}^L \norm{\Theta^{l}}^2
  \]
  where the loss fucntion $L$ can be cross-entropy loss, squared error, etc.
\item Then, to learn, we minimise $J$ using gradient descent. To calculate the
  derivatives of $\Theta_{i, j}^{(l)}$, we need to calculate them for the layer
  after them, and then use the chain rule.
\item It is crucial to randomly initialise the weights of the network to prevent
  two units learning the same thing in the network.
\item During implementation, use gradient checking (numerically approximate the
  derivatives and compare them with analytical results).
\end{itemize}
\subsection{Deep Learning}
Classical ML pipeline:
\[
  \text{data} \underbrace{\to}_{\text{feature extraction}} \vec{x}
  \underbrace{\to}_{\text{feature mapping}} h_m(\vec{x})
  \underbrace{\to}_{\text{(linear) model}} \hat{\vec{y}}
\]
\paragraph{Deep Learning:} Any algorithm that attempts to learn intermediate
representations of the data.
\paragraph{Problems of DL:}
\begin{itemize}
\item Computational Complexity
\item Availability of data: we need a lot of it to learn a lot of parameters correctly
\item Overfitting
\item (Highly) Non-convex cost function
\item Vanishing gradient problem: 
  if we have a sigmoid function $\sigma$, then if $0 << |x|$ then $\sigma'(x)
  \approx \sigma'(x \pm h)$ for small $h > 0$, meaning that it is hard for the
  network to learn. \\
  \textbf{Solution:} use ReLU, leaky ReLU or Softplus. 
\item Picking a network topology
\end{itemize}
\subsection{Regularisation techniques}
\begin{itemize}
\item Get a lot of data: this has the best regularisation effect
\item Data augmentation: modify preexisting data sensibly to improve learning
  outcome: e.g. add noise, rotate image, etc.
\item Weight regularisation:
  \[
    J(\Theta) = L(\Theta) + \underbrace{\frac{\lambda}{2}\sum_{\substack{i,j,l
          \\ i \neq 0}}
      (\Theta_{i, j}^{(l)})^2}_{\text{regularisation term}}
  \]
\item Early stopping: if the loss function stagnates, stop the learning process
  to stop overfitting because the \textbf{validation loss might increase}!
\item Dropout: for each unit, do not consider it with some probability $0 < p <
  1$. This adds natural noise, as we're training only on partial data and forces
  the network to be much more general.
\item Stochastic / Minibatch Gradient Descent: instead of calculating gradients
  based on the entire dataset, introduce noise by shuffling the dataset,
  splitting it up into smaller batches and calculating gradients based on the
  smaller batches
  \paragraph{Note:} we need to decrease the learning rate otherwise we will
  never converge. This is called \textbf{weight decay}.
  \paragraph{Note:} SGD improves behaviour around saddle points / local minima.
\item Batch normalisation: after the activation of every layer, normalise the
  activations! Speeds up training and has a regularisation effect.
\end{itemize}
\paragraph{Weight sharing:} instead of learning a bunch of different stuff, we
could share some weights along nodes and help them optimise the features
extracted by those shared weights. Leads to CNNs.
\paragraph{Max-pooling:} reduce the dimensionality of the data by throwing away
locally insignificant data.
\paragraph{Recent DL successes:} AlexNet, ResNet
\section{Unsupervised Learning}
\subsection{K-means clustering}
\begin{enumerate}
\item Start with $K$ randomly initialised centroids $\mu_i$
\item During each iteration, find the closest centroid to each data point
  \[
    c_i \leftarrow \min_{k \in \{1,\hdots, K\}} \norm{\mu_i - \vec{x}_i}
  \]
\item set the centroids to the means of their assigned datapoints
  \[
    \mu_k \leftarrow \frac{1}{\text{(\# of datapoints in cluster i)}} \sum_{c_i
      = k} x_i
  \]
\item Repeat 2. and 3. until convergence
\end{enumerate}
To evaluate the clustering algorithm, we can use the loss function
\[
  J(\vec{c}, \vec{\mu}) = \frac{1}{m} \sum_{i = 1}^m \norm{\vec{x}_i - \mu_{c_i}}^2,
\]
i.e. we want to minimise the total mean distance.
Then, to find the best clustering, run the algorithm 100 times, say, and use the
one that minimises $J$.
\subsection{PCA}
Dimensionality reduction: calculate a new orthogonal basis for the data and
throw away axes along which the variance is low
\end{document}